# COMPUTATIONALLY EFFICIENT LOCALIZED ATTENTION
In this work, we propose 3 different mechanisms to represent the input sequence using different levels, such that the length of vectors in the subsequent level decreases by a constant factor. Each level contains coarser details than the previous level. 
Based on the top level representation, the model will attend to particular parts of the input sequence, following a particular path dynamically.
For more details read up the report.pdf
